<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="TrafficLoc: Localizing Traffic Surveillance Cameras in 3D Scenes">
  <meta property="og:title" content="TrafficLoc: Localizing Traffic Surveillance Cameras in 3D Scenes"/>
  <meta property="og:description" content="TrafficLoc: Localizing Traffic Surveillance Cameras in 3D Scenes"/>
  <meta property="og:url" content="https://tum-luk.github.io/projects/trafficloc/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="TrafficLoc, 3D Localization">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TrafficLoc: Localizing Traffic Surveillance Cameras in 3D Scenes</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <style>
    .button.is-light.is-disabled {
      background-color: #f5f5f5; /* 浅灰色背景 */
      color: #b5b5b5;           /* 图标和文本的灰色 */
      border: none;             /* 无边框 */
      pointer-events: none;     /* 禁止点击 */
      opacity: 1;               /* 确保不透明度一致 */
    }

    .button.is-light.is-disabled .icon i {
      color: #b5b5b5; /* 确保图标是灰色 */
    }
  </style>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">TrafficLoc: Localizing Traffic Surveillance Cameras in 3D Scenes</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yan-xia.github.io/" target="_blank">Yan Xia</a><sup>1,2*&dagger;</sup>,</span>
              <span class="author-block">
                <a href="https://tum-luk.github.io/" target="_blank">Yunxiang Lu</a><sup>2*</sup>,</span>
              <span class="author-block">
                <a href="https://rruisong.github.io/" target="_blank">Rui Song</a><sup>2,4</sup>,</span>
              <span class="author-block">
                <a href="https://cvg.cit.tum.de/members/dhou" target="_blank">Oussema Dhaouadi</a><sup>2,5</sup>,</span>
              <span class="author-block">
                <a href="https://www.robots.ox.ac.uk/~joao/" target="_blank">João F. Henriques</a><sup>6</sup>,</span>
              <span class="author-block">
                <a href="https://cvg.cit.tum.de/members/cremers" target="_blank">Daniel Cremers</a><sup>2,3</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Science and Technology of China</span>
              <span class="author-block"><sup>2</sup>Technical University of Munich</span>
              <span class="author-block"><sup>3</sup>Munich Center for Machine Learning (MCML)</span>
              <span class="author-block"><sup>4</sup>Fraunhofer IVI</span>
              <span class="author-block"><sup>5</sup>DeepScenario</span>
              <span class="author-block"><sup>6</sup>Visual Geometry Group, University of Oxford</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <!-- <span class="link-block">
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span> -->
              
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.10308" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/TUM-Luk/TrafficLoc" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop width="100%">
        <!-- Your video here -->
        <source src="static/videos/result1.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Localization result on Carla Intersection Dataset
      </h2>
      <!-- <br> -->
      <video poster="" id="tree" autoplay controls muted loop width="100%">
        <!-- Your video here -->
        <source src="static/videos/result2.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Localization result on Carla Intersection Dataset
      </h2>
      <!-- <br> -->
      <video poster="" id="tree" autoplay controls muted loop width="100%">
        <!-- Your video here -->
        <source src="static/videos/result3.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Localization result on KITTI Dataset
      </h2>
      <!-- <br> -->
      <video poster="" id="tree" autoplay controls muted loop width="100%">
        <!-- Your video here -->
        <source src="static/videos/result4.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Localization result on NuScenes Dataset
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container">
  <div class="hero-body">
  <div class="grid-container">
    <div class="grid-item">
      <video poster="" id="tree" autoplay controls muted loop width="50%">
        <source src="static/videos/result_1.mp4"
        type="video/mp4">
        </video>
    </div>
    <div class="grid-item">
      <video poster="" id="tree" autoplay controls muted loop width="50%">
        <source src="static/videos/result_1.mp4"
        type="video/mp4">
      </video>
    </div>
    <div class="grid-item">
      <video poster="" id="tree" autoplay controls muted loop width="50%">
        <source src="static/videos/result_1.mp4"
        type="video/mp4">
      </video>
    </div>  
    <div class="grid-item">
      <video poster="" id="tree" autoplay controls muted loop width="50%">
        <source src="static/videos/result_1.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</div>
</div>
</section> -->

<!-- Paper abstract -->
<!-- <section class="section hero is-light"> -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We tackle the problem of localizing the traffic surveillance cameras in cooperative perception. 
            To overcome the lack of large-scale real-world intersection datasets, we introduce <strong>Carla Intersection</strong>, a new simulated dataset with <strong>75</strong> urban and rural intersections in Carla. 
            Moreover, we introduce a novel neural network, TrafficLoc, localizing traffic cameras within a 3D reference map. 
            TrafficLoc employs a coarse-to-fine matching pipeline. For image-point cloud feature fusion, we propose a novel Geometry-guided Attention Loss to address cross-modal viewpoint inconsistencies. 
            During coarse matching, we propose an Inter-Intra Contrastive Learning to achieve precise alignment while preserving distinctiveness among local intra-features within image patch-point group pairs. 
            Besides, we introduce Dense Training Alignment with a soft-argmax operator to consider additional features when regressing the final position. 
            Extensive experiments show that our TrafficLoc improves the localization accuracy over the state-of-the-art Image-to-point cloud registration methods by a large margin <strong>(up to 86%)</strong> on Carla Intersection and generalizes well to real-world data. 
            TrafficLoc also achieves new SOTA performance on KITTI and NuScenes datasets, demonstrating strong localization ability across both in-vehicle and traffic cameras.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop width="100%">
        <!-- Your video here -->
        <source src="static/videos/forward.mp4"
        type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered">
        TrafficLoc
      </h2> -->
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- <div align="center" style="margin: 80px 0 120px 0;">
  <img style='height: auto; width: 25%; object-fit: contain' src="static/images/ustc_result.png" alt="TrafficLoc Pipeline">
</div> -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Method</h2>
        <!-- <h3 class="title is-4">LEAP Front-end</h3> -->
        <div class="content has-text-justified">
          <p>
            Taking a pair of 3D point cloud and a 2D image as input, TrafficLoc first performs feature extraction to obtain features in point group level and image patch level. 
            The <strong>Geometry-guided Feature Fusion (GFF)</strong> module strengthens the feature and then match them based on similarity rule. 
            Fine features are extracted based on the coarse matching results and fine matching is performed between the point group center and the extracted image window with a soft-argmax operation. 
            The final generated 2D-3D correspondences are utilized to optimize the camera pose with RANSAC+EPnP algorithm.
          </p>
        </div>
        <img src="static/images/draft_new.png"
                  class="teaser-fig"
                  width="99%"
                  alt="teaser-fig."/>
        <p class="is-size-16 has-text-black">
          Figure 1: Pipeline of our proposed TrafficLoc for relocalization
        </p>
        <br>
        <div class="content has-text-justified">
          <p>
            Our <strong>Geometry-guided Feature Fusion (GFF) module</strong> first uses multiple layers of self and cross-attention module to enhance the feature across different modalities (left). 
            The <strong>Geometry-guided Attention Loss (GAL)</strong> is applied to the cross-attention map of the last fusion layer based on camera projection geometry (right).
          </p>
        </div>
        <img src="static/images/draft_att_new.png"
                  class="teaser-fig"
                  width="99%"
                  alt="teaser-fig."/>
        <p class="is-size-16 has-text-black">
          Figure 2: Pipeline of Geometry-guided Feature Fusion (GFF) module
        </p>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Carla Intersection Dataset</h2>
        <!-- <h3 class="title is-4">LEAP Front-end</h3> -->
        <div class="content has-text-justified">
          <p>
            We set up a new simulated intersection dataset, <strong>Carla Intersection</strong>, to study the traffic camera localization problem in varying environment.
            Carla Intersection dataset comprises 75 intersections across 8 worlds within the Carla simulation environment, encompassing urban and rural landscapes. 
            We use on-board LiDAR sensor to capture point cloud scans, which are then accumulated and downsampled to get the 3D point cloud of the intersection. 
            For each intersection, we captured 768 training images and 288 testing images with known 6-DoF pose at a resolution of 1920x1080 pixel and a horizontal field of view (FOV) of 90◦. 
            In consideration of real-world traffic surveillance camera installations, our image collection spans heights from 6 to 8 meters, with camera pitch angles from 15 to 30 degrees. 
            This setup reflects typical positioning to capture optimal traffic views under varied monitoring conditions. 
            More details of our dataset are in the paper.
          </p>
        </div>
      </div>
    </div>

  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/carla_t1int1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Point cloud and example images from <strong>Town01 Int1</strong>
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carla_t2int7.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Point cloud and example images from <strong>Town02 Int7</strong>
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/carla_t3int4.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Point cloud and example images from <strong>Town03 Int4</strong>
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/carla_t4int5.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Point cloud and example images from <strong>Town04 Int5</strong>
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/carla_t5int7.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Point cloud and example images from <strong>Town05 Int7</strong>
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/carla_t6int7.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Point cloud and example images from <strong>Town06 Int7</strong>
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/carla_t7int2.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Point cloud and example images from <strong>Town07 Int2</strong>
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/carla_t10int1.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Point cloud and example images from <strong>Town10 Int1</strong>
      </h2>
    </div>
  </div>
</div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Exp. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Qualitative Localization Results</h2>
        <div class="content has-text-justified">
          <p>
            Qualitative localization results on the proposed Carla Intersection dataset and KITTI Odometry dataset.
            The point cloud is projected into a 2D view and shown above the image, with point colors indicating distance. 
            The proposed TrafficLoc achieves better performance, with more correct (green) and fewer incorrect (red) point-to-pixel pairs. 
            The first column presents the input point cloud and input image.
          </p>
        </div>
        <img src="static/images/supp_result.png"
                class="teaser-fig"
                width="99%"
                alt="teaser-fig."/>
        <p class="is-size-16 has-text-black">
          Figure 3: Qualitative results of our TrafficLoc and other baseline methods. 
          (a), (b) and (c) show results on the Carla Intersection Dataset.
          (d) shows results on the KITTI Odometry dataset.
          <!-- (a1) and (a2) show predicted correspondences on <strong>unseen</strong> intersection from seen world style.  -->
          <!-- (b1) and (b2) show results on <strong>unseen</strong> intersection from seen world style with viewpoint variation.  -->
          <!-- (c1) and (c2) show results on <strong>unseen</strong> intersection from <strong>unseen</strong> world style.  -->
        </p>
      </div>
    </div>
    <!--/ Exp. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Exp. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Ablation Visualization</h2>
        <div class="content has-text-justified">
          <p>
            We visualize the cross-attention map between two modalities. 
            With the use of <strong>Geometry-guided Attention Loss (GAL)</strong>, the P2I attention map of point group P<sub>3D</sub> tends to concentrate more on the image region where the point group is projected, 
            while the I2P attention map for patch I<sub>2D</sub> assigns greater weights to the area traversed by the camera ray of this patch. 
            Both observations highlight the geometry-awareness of our proposed GAL.
          </p>
        </div>
        <img src="static/images/ablation_att.png"
                class="teaser-fig"
                width="60%"
                alt="teaser-fig."/>
        <p class="is-size-16 has-text-black">
          Figure 4: Visualization result of P2I and I2P attention map when using <strong>Geometry-guided Attention Loss (GAL)</strong> or not. 
          Red color indicates high attention value and blue means low value.
          <!-- (a1) and (a2) show predicted correspondences on <strong>unseen</strong> intersection from seen world style.  -->
          <!-- (b1) and (b2) show results on <strong>unseen</strong> intersection from seen world style with viewpoint variation.  -->
          <!-- (c1) and (c2) show results on <strong>unseen</strong> intersection from <strong>unseen</strong> world style.  -->
        </p>
      </div>
    </div>
    <!--/ Exp. -->
  </div>
</section>




<!-- Youtube video -->
<!--
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">

      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">

            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
-->
<!-- End youtube video -->


<!-- Video carousel -->
<!--
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\

            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
-->
<!-- End video carousel -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@string{iccv="IEEE International Conference on Computer Vision (ICCV)"}
      @inproceedings{xia2025trafficloc,
      title = {TrafficLoc: Localizing Traffic Surveillance Cameras in 3D Scenes},
      author = {Y Xia and Y Lu and R Song and O Dhaouadi and JF Henriques and D Cremers},
      booktitle = {IEEE International Conference on Computer Vision (ICCV)},
      year = {2025},
      titleurl = {trafficloc.png},
      keywords = {3D Localization, Traffic Surveillance Camera, LiDAR point cloud},
      url = {https://tum-luk.github.io/projects/trafficloc/},
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
